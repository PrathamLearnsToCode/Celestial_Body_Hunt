# -*- coding: utf-8 -*-
"""high_energy_gamma_particles.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rs-9pmCnqb4pTovoaxCjyIJjrTQ49Fiq

## Importing the Libraries
"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout
import warnings
warnings.filterwarnings('ignore')

"""## Importing the CSV file"""

df = pd.read_csv('telescope_data.csv')
df.head()

"""## Checking for missing values and Datatype"""

df_new = df.drop(['Unnamed: 0'], axis = 1)
df_new.columns

df_new.describe()

df_new.info()

df_new.isnull().sum()

"""## Data Visualization

Box method to visualize the Median of Data
"""

features = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long','fM3Trans', 'fAlpha', 'fDist']
sns.set_style("darkgrid")
for i,t in enumerate(features):
    sns.catplot(y=t, 
                x= "class",
                data=df_new, 
                orient='v', 
                ax=axes[i%2], 
                palette = 'Dark2_r',
                kind = 'box',
                order = ['h','g'],
                height = 5)
plt.show()

"""Violin method to visualize the Median of Data"""

features = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long','fM3Trans', 'fAlpha', 'fDist']
sns.set_style("darkgrid")
for i,t in enumerate(features):
    sns.catplot(y=t, 
                x= "class",
                data=df_new, 
                orient='v', 
                ax=axes[i%2], 
                palette = 'magma',
                kind = 'violin',
                order = ['h','g'],
                height = 5)
plt.show()

"""Boxen
 method to visualize the Median of Data
"""

features = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long','fM3Trans', 'fAlpha', 'fDist']
sns.set_style("darkgrid")
for i,t in enumerate(features):
    sns.catplot(y=t, 
                x= "class",
                data=df_new, 
                orient='v', 
                ax=axes[i%2], 
                palette = "viridis",
                kind = 'boxen',
                order = ['h','g'],
                height = 5)
plt.show()

"""Histogram Plot of Data"""

df_new.hist(bins = 10, figsize = (20,20), color='orange')
plt.show()

"""Distplot of Data"""

plt.rcParams['figure.figsize'] = (20, 9)
sns.distplot(df_new['fLength'])
plt.show()

plt.rcParams['figure.figsize'] = (20, 9)
sns.distplot(df_new['fWidth'])
plt.show()

plt.rcParams['figure.figsize'] = (20, 9)
sns.distplot(df_new['fSize'])
plt.show()

plt.rcParams['figure.figsize'] = (20, 9)
sns.distplot(df_new['fConc'])
plt.show()

plt.rcParams['figure.figsize'] = (20, 9)
sns.distplot(df_new['fConc1'])
plt.show()

plt.rcParams['figure.figsize'] = (20, 9)
sns.distplot(df_new['fAsym'])
plt.show()

plt.rcParams['figure.figsize'] = (20, 9)
sns.distplot(df_new['fM3Long'])
plt.show()

plt.rcParams['figure.figsize'] = (20, 9)
sns.distplot(df_new['fM3Trans'])
plt.show()

plt.rcParams['figure.figsize'] = (20, 9)
sns.distplot(df_new['fAlpha'])
plt.show()

plt.rcParams['figure.figsize'] = (20, 9)
sns.distplot(df_new['fDist'])
plt.show()

"""Pairplot to visualize the Data"""

sns.pairplot(data=df_new,hue = 'class')
plt.tight_layout()
plt.show()

"""## Assigning the features,labels and performing Categorical Encoding"""

x = df_new.iloc[:,:-1].values
y = df_new.iloc[:,-1].values
le = LabelEncoder()
y = le.fit_transform(y)
print(y)

"""Splitting the Dataset into Training and Testing"""

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.20,random_state = 0)

"""## Feature Scaling"""

sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

"""## Data Modeling"""

ann = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation='relu'),
    
    tf.keras.layers.Dense(512, activation='relu'),
    
    tf.keras.layers.Dense(256, activation='relu'),
    
    tf.keras.layers.Dense(1, activation='sigmoid'),
])

ann.compile(optimizer = 'adam',loss= 'binary_crossentropy',metrics = ['accuracy'])

history = ann.fit(x_train,y_train,validation_data = (x_test,y_test),epochs = 10,batch_size = 32)

"""## Visualizing the Results"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['Training', 'Validation'])
plt.title("Losses")
plt.xlabel('epoch')

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['Training', 'Validation'])
plt.title("Training and validation accuracy")
plt.xlabel('epoch')